<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning on Nitin Patil</title>
    <link>/machine_learning/</link>
    <description>Recent content in Machine Learning on Nitin Patil</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 01 Jan 2019 19:14:46 +0530</lastBuildDate>
    
	<atom:link href="/machine_learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Image Classification</title>
      <link>/machine_learning/image_classification/</link>
      <pubDate>Sun, 31 Mar 2019 18:05:20 +0530</pubDate>
      
      <guid>/machine_learning/image_classification/</guid>
      <description>Image Classification is a task of assigning an input image one label from a fixed set of categories. Many Computer Vision tasks (such as object detection, segmentation) can be reduced to image classification.
For computer images are 3-dimensional arrays of integers from 0 (black) to 255 (white) representing brightness, of size [Width x Height x 3]. The 3 represents the three color channels Red, Green, Blue.
The task in Image Classification is to predict a single label (or a distribution over labels as shown here to indicate our confidence) for a given image.</description>
    </item>
    
    <item>
      <title>k-Nearest Neighbor Classifier</title>
      <link>/machine_learning/k-nearest_neighbor_classifier/</link>
      <pubDate>Sun, 31 Mar 2019 18:05:20 +0530</pubDate>
      
      <guid>/machine_learning/k-nearest_neighbor_classifier/</guid>
      <description>To understand the algorithm let&amp;rsquo;s consider a problem of image classification, in which we have an image of 32 x 32 x 3 pixel and want to assign a single label from a fixed set of categories.
Nearest Neighbor Classifier The learning step of nearest neighbor classifier is very simple. It just stores all the dataset to compare with test image.
To predict it takes a test image, compare it to every single one of the training images, and predict the label of the closest training image.</description>
    </item>
    
    <item>
      <title>Decision tree algorithm</title>
      <link>/machine_learning/decision_tree_algorithm/</link>
      <pubDate>Fri, 25 Jan 2019 19:14:46 +0530</pubDate>
      
      <guid>/machine_learning/decision_tree_algorithm/</guid>
      <description>Here we will implement the Decision Tree algorithm and compare our algorithm&amp;rsquo;s performance with decision trees from sklearn.tree. Purpose of this excercise is to write minimal implementation to understand how theory becomes code, avoiding layers of abstraction.
Decision Trees are a non-parametric supervised learning method used for classification and regression. The goal of decision tree is to learn simple decision rules from training data and use those rules to predict target value for test data.</description>
    </item>
    
    <item>
      <title>Ensemble, bagging and random forest</title>
      <link>/machine_learning/ensemble_random_forest/</link>
      <pubDate>Fri, 25 Jan 2019 19:14:46 +0530</pubDate>
      
      <guid>/machine_learning/ensemble_random_forest/</guid>
      <description>Ensemble An ensemble is a set of elements that collectively contribute to a whole. A familiar example is a musical ensemble, which blends the sounds of several musical instruments to create harmony, or architectural ensembles. In ensembles, the (whole) harmonious outcome is more important than the performance of any individual part.
In machine learning the goal of ensembling is to combine the predictions of several base estimators built with a given learning algorithm in order to improve generalizability / robustness over a single estimator.</description>
    </item>
    
    <item>
      <title>Feature Selection</title>
      <link>/machine_learning/feature-selection/</link>
      <pubDate>Fri, 25 Jan 2019 19:14:46 +0530</pubDate>
      
      <guid>/machine_learning/feature-selection/</guid>
      <description>Feature selection is very important step in machine learning. In this step we are suppoesed to select features which are giving high model score and drop features reponsible for model score reduction.
There are different techniques used for feature selection. - Use features with corelation (either positive or negative) with target - Train a model and select features with high importance - Use different feature selection algorithms
RFECV Feature ranking with recursive feature elimination and cross-validated selection of the best number of features.</description>
    </item>
    
    <item>
      <title>Gradient boosting machines</title>
      <link>/machine_learning/gradient_boosting_machines/</link>
      <pubDate>Fri, 25 Jan 2019 19:14:46 +0530</pubDate>
      
      <guid>/machine_learning/gradient_boosting_machines/</guid>
      <description>In machine learning we are given a set of data points and goal is to create a function that draws nice curve through the data points. We call that function a model and it maps $X$ to $y$, thus, making predictions given some unknown $x$. Adding up a bunch of subfunctions to create a composite function is called additive modeling. Gradient boosting machines use additive modeling to gradually nudge an approximate model towards a really good model, by adding simple submodels to a composite model.</description>
    </item>
    
    <item>
      <title>Unit testing</title>
      <link>/machine_learning/unit_testing/</link>
      <pubDate>Fri, 25 Jan 2019 19:14:46 +0530</pubDate>
      
      <guid>/machine_learning/unit_testing/</guid>
      <description>We should write the test cases to test our software. These tests ensures no regression in existing functionality while adding new changes to the code.
pytest pytest is a framework that makes building simple and scalable tests easy. Tests are expressive and readableâ€”no boilerplate code required. pytest test cases are a series of functions in a Python file starting with the name test_.
Install Run the following command in your command line:</description>
    </item>
    
    <item>
      <title>Visualize Decision Tree</title>
      <link>/machine_learning/visualize_decision_tree/</link>
      <pubDate>Fri, 25 Jan 2019 19:14:46 +0530</pubDate>
      
      <guid>/machine_learning/visualize_decision_tree/</guid>
      <description>import os # On Windows you need to set your path to graphviz (C:\\Program Files (x86)\\Graphviz2.38\\bin\\ for example)  path_to_graphviz = &amp;#39;C:\\Program Files (x86)\\Graphviz2.38\\bin\\&amp;#39; os.environ[&amp;#34;PATH&amp;#34;] += os.pathsep + path_to_graphviz Train a classifier on iris dataset
from sklearn.datasets import load_iris from sklearn import tree iris = load_iris() clf = tree.DecisionTreeClassifier(max_depth=3) clf = clf.fit(iris.data, iris.target)import graphviz # Calling `export_graphviz` with `out_file=None` returns GraphViz representation (DOT format) of the decision tree as string.</description>
    </item>
    
  </channel>
</rss>