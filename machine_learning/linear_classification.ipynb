{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Linear classification\"\n",
    "date: 2019-04-01T18:05:20+05:30\n",
    "draft: false\n",
    "author: \"Nitin Patil\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the algorithm let's consider a problem of [image classification](/machine_learning/image_classification/), in which we have an image of 32 x 32 x 3 pixel and want to assign a single label from a fixed set of categories.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear classification also known as Logistic regression is a powerful approach for image classification. It does a parameterized mapping of input to the outputs. It is naturally extend to Neural Networks and Convolutional Neural Networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **score function** maps the raw data $X$ to class scores $y$. \n",
    "\n",
    "A **loss function** quantifies the agreement between the predicted scores $\\hat y$ and the ground truth labels $y$.\n",
    "\n",
    "In **optimization** we will minimize the loss function with respect to the parameters $W$ of the score function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score function\n",
    "### Parameterized mapping from images to label scores\n",
    "\n",
    "Define the score function that maps the pixel values of an image to confidence scores for each class.\n",
    "\n",
    "We have a training dataset of images $x_i \\in R^D$, each associated with a label $y_i$. Here $i = 1 \\dots N$ and $y_i \\in { 1 \\dots K }$. That is, we have $N$ examples (each with a dimensionality $D$) and $K$ distinct categories.\n",
    "\n",
    "e.g. in CIFAR-10, $N$ = 50,000 images, each with $D$ = 32 x 32 x 3 = 3072 pixels, and $K$ = 10 classes.\n",
    "\n",
    "now the score function would be $f: R^D \\mapsto R^K$ which maps the raw image pixels to class scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear mapping\n",
    "\n",
    "$$f(x_i, W, b) =  W x_i + b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x_i$ is a image with all of its pixels flattened out to a single column vector of shape [D x 1]\n",
    "\n",
    "The matrix $W$ (of size [K x D]), and the vector $b$ (of size [K x 1]) are the **weights or parameters** of the function. $b$ is called bias vector as the influence the output score without interacting with actual input data $x_i$. \n",
    "\n",
    "[10 x 3072] [3072 x 1] + [10 x 1] = [10 x 1]\n",
    "\n",
    "In CIFAR-10, 3072 numbers come into the function (the raw pixel values) and 10 numbers come out (the class scores)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider this toy example where we want to classify image in three classes.\n",
    "\n",
    "![linear_classifier](linear_classifier.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "- $W$ contains 10 different classifiers one for each class. Each row of $W$ is a single classifier. $W x_i$ is evaluation of all 10 classifiers in parallel.\n",
    "\n",
    "- Our goal is to learn the $W,b$ such that the score function will predict the labels as close to the ground truth labels across the whole training set. The correct class score should be higher than incorrect class score.\n",
    "\n",
    "- Once the weights are learned we can be discard the entire training data. To classify the test image it just have to forwarded through the score function.\n",
    "\n",
    "- classification is significantly faster as just has matrix multiplication and addition.\n",
    "\n",
    "    Convolutional Neural Networks will map image pixels to scores exactly like linear classifier, but the mapping function will be more complex and will contain more parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation of linear classifiers as template matching\n",
    "\n",
    "Each row of $W$ corresponds to a template (or sometimes also called a prototype) for one of the classes.\n",
    "\n",
    "The input image is matched (score) with each template (using dot product) and the class of template which \"fits\" best is the predicted class.\n",
    "\n",
    "With this terminology, the linear classifier is doing template matching, where the templates $W$ are learned.\n",
    "\n",
    "![weight_templates](weight_templates.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As linear classifier has a single template for each class, everything the classifier learns about a class from training example is merged into those single template.\n",
    "\n",
    "Like horse template became two headed horse because dataset has some horses with head on left side and some on right side.\n",
    "\n",
    "As most of the cars in dataset are of red color, car template ended up being red. So the linear classifier is too weak to properly account for different-colored cars. \n",
    "\n",
    "By introducing hidden layers neural network could effectively classify the different colored car."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias trick\n",
    "\n",
    "![bias_trick](bias_trick.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the bias vector in $b$ in $W$ which resize it to [K x D+1].\n",
    "Also extend the $x_i$ vector with one additional dimension that always holds constant 1 for bias dimension. So $x_i$ resizes to [D+1, 1]\n",
    "\n",
    "With this new score function reduced to single matrix multiply.\n",
    "\n",
    "$$f(x_i, W) =  W x_i$$\n",
    "\n",
    "CIFAR-10 dimensions will be [10 x 3073] [3073 x 1] = [10 x 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image data preprocessing\n",
    "The raw pixel values range from [0…255]. \n",
    "In Machine Learning, it is a very common practice to always perform normalization of your input features (in the case of images, every pixel is thought of as a feature). \n",
    "\n",
    "**center the data** by subtracting mean from every feature. \n",
    "\n",
    "Compute mean image across the training images and substract it from every image to center pixel range to [-127 … 127]\n",
    "\n",
    "Further **scale** each input feature so that its values range from [-1, 1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function or Cost function or Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In score function we have set the $W$ to predict the class scores for input image. Now we want to compare the class score with the actual ground truth class. We expect the class score for the correct class to be higher than the incorrect class scores. This loss function measure our unhappiness with the outcome. Intuitively, the loss will be high if we’re doing a poor job of classifying the training data, and it will be low if we’re doing well.\n",
    "\n",
    "Let's see different loss functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass Support Vector Machine loss\n",
    "\n",
    "The SVM “wants” the correct class for each image to have a higher score than the incorrect classes by some fixed margin $\\Delta$. \n",
    "\n",
    "SVM would be happy if it get what it wants and this happiness would yield a lower loss (which is good).\n",
    "\n",
    "Loss for $i^{th}$ image\n",
    "\n",
    "$$L_i = \\sum_{j\\neq y_i} \\max(0, s_j - s_{y_i} + \\Delta)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $i$ - image index in $N$\n",
    "- $j$ - class index in $K$\n",
    "- $L_i$ - loss for $i^{th}$ image\n",
    "- $s_j$ - score for $j^{th}$ class $s_j = f(x_i, W)_j$\n",
    "- $y_i$ - actual class $y_i \\in { 1 \\dots K }$\n",
    "- $s_{y_i}$ - score for actual class $y_i$ class $s_{y_i} = f(x_i, W)_{y_i}$\n",
    "- $\\Delta$ - margin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- http://cs231n.github.io/linear-classify/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
