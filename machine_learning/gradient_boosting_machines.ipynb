{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Gradient boosting machines\"\n",
    "date: 2019-01-25T19:14:46+05:30\n",
    "draft: False\n",
    "author: \"Nitin Patil\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning we are given a set of data points and goal is to create a function that draws nice curve through the data points. We call that function a model and it maps $X$ to $y$, thus, making predictions given some unknown $x$. Adding up a bunch of subfunctions to create a composite function is called additive modeling. Gradient boosting machines use additive modeling to gradually nudge an approximate model towards a really good model, by adding simple submodels to a composite model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "\n",
    "Boosting combines multiple weak models sequentially into a single composite model. The idea is that, as we introduce more weak models, the overall model becomes a stronger and stronger predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient boosting machines (GBM)\n",
    "\n",
    "![PNG](golf-dir-vector.png)\n",
    "\n",
    "\n",
    "Lets consider boosting approach as a golf play. Golfer is initially hitting a  ball towards the hole at $y$ but only getting as far as $f_0(X)$. The golfer then repeatedly taps the ball more softly, working the ball towards the hole, after reassessing direction and distance to the hole at each stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GBM for regression pseudocode\n",
    "\n",
    "1. Train first model $f_0(X)$ to learn target $y$ given $X$. It predicts average of $y$ values in leaf nodes.\n",
    "- Compute the difference between target $y$ and the prediction of first model, $y - f_0(X)$ it is called *residual* or *direction vector*. \n",
    "- Train next weak model $\\Delta\\_0(X)$ on residual of previous models i.e. $y - f_0(X)$\n",
    "- The new prediction model $F_1(X)$ is the addition of previous model and a nudge, $\\Delta\\_1(X)$, multiplied by learning rate $\\eta$\n",
    "    $$F\\_1(X) = f_0(X) + \\eta \\Delta\\_1(X)$$\n",
    "- Likewise train every subsequent weak model $\\Delta\\_m(X)$ on residual of previous models i.e. $y - F\\_{m-1}(X)$\n",
    "- Adding together all weak models with initial $f_0(X)$ gives a strong composite model $F_M(X)$.\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray\\*}\n",
    "\\hat y & = & f_0(X) + \\eta \\Delta\\_1(X) + \\eta \\Delta\\_2(X) + ...  +  \\eta \\Delta\\_M(X) \\\\\\\\\n",
    " & = & f\\_0(X) + \\eta \\sum\\_{m=1}^M  \\Delta_m(X) \\\\\\\\\n",
    " & = & F_M(X) \\\\\\\\\n",
    "\\end{eqnarray\\*}\n",
    "$$\n",
    "\n",
    "Or using recurrence\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray\\*}\n",
    "F_0(X) &=& f_0(X)\\\\\n",
    "F\\_m(X) &=& F\\_{m-1}(X) + \\eta \\Delta\\_m(X)\\\\\n",
    "\\end{eqnarray\\*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two common direction vector choices\n",
    "\n",
    "- When $\\Delta\\_m(X)$ models are trained on residual, $y - F_{m-1}(X)$, the overall model gets optimized for squared error loss function.\n",
    "\n",
    "\n",
    "- When $\\Delta\\_m(X)$ models are trained on sign, $sign (y - F_{m-1}(X))$, the overall model gets optimized for absolute error loss function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameters\n",
    "\n",
    "- number of stages $M$ - The more stages we use, the more accurate the model, but the more likely we are to be overfitting.\n",
    "\n",
    "- learning rate $\\eta$ - The primary purpose of the learning rate, or “shrinkage” as some papers call it, is to reduce overfitting of the overall model.\n",
    "\n",
    "As Chen and Guestrin say in [XGBoost: A Scalable Tree Boosting System](https://arxiv.org/pdf/1603.02754.pdf), “shrinkage reduces the influence of each individual tree and leaves space for future trees to improve the model.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- https://explained.ai/gradient-boosting/index.html\n",
    "- [XGBoost: A Scalable Tree Boosting System](https://arxiv.org/pdf/1603.02754.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
